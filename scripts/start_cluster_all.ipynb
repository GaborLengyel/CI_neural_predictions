{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c210af42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "ppath = \"/Users/glengyel/Library/Mobile Documents/com~apple~CloudDocs/Projects/Neural_link_of_CI/neural_link/results/\"\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import h5py\n",
    "\n",
    "#import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.optim as optim\n",
    "#from torch.utils.data import DataLoader, TensorDataset\n",
    "#import numpy as np\n",
    "#from scipy.stats import entropy\n",
    "#from scipy.stats import zscore\n",
    "#from sklearn.datasets import fetch_olivetti_faces\n",
    "#from sklearn.cluster import KMeans\n",
    "#from scipy.spatial.distance import cdist\n",
    "#from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "#from sklearn.cluster import AgglomerativeClustering\n",
    "#from sklearn.cluster import DBSCAN\n",
    "#import random\n",
    "#from skimage.transform import resize\n",
    "\n",
    "clist = []\n",
    "for cname in [\"winter\", \"spring\"]:\n",
    "    c = cm.get_cmap(cname, 100)\n",
    "    clist.append(ListedColormap(c(np.linspace(0, 1, 10))))\n",
    "\n",
    "p = os.getcwd()\n",
    "Pth = os.path.dirname(p)\n",
    "FigPath= \"/plots/\"\n",
    "saveFigPath = Pth+FigPath\n",
    "\n",
    "FigParams = {\n",
    "    \"fig_size\": 6,\n",
    "    \"font1\": {\"fontname\": \"Arial\", \"size\": 32},\n",
    "    \"font2\": {\"fontname\": \"Arial\", \"size\": 36},\n",
    "    \"line_w\": 2,\n",
    "    \"colors_maps\": clist,\n",
    "    \"colors_names\": [\"royalblue\", \"olivedrab\", \"darkorange\", \"firebrick\"],\n",
    "    \"saveFigPath\": saveFigPath,\n",
    "    \"ErrorBar\": {'ErrDist':[0.4,0.4],\n",
    "                 'ErrSize':1,\n",
    "                 'ErrWid':4,\n",
    "                 'sizedots':10,\n",
    "                 'ErrColor':[\"royalblue\", \"olivedrab\", \"darkorange\", \"firebrick\"],\n",
    "                 'DotsColor':[\"royalblue\", \"olivedrab\", \"darkorange\", \"firebrick\"],\n",
    "                 'sizeMean':25},\n",
    "}\n",
    "\n",
    "\n",
    "def packParams():\n",
    "    return (FigParams[\"fig_size\"],\n",
    "            FigParams[\"font1\"],\n",
    "            FigParams[\"font2\"],\n",
    "            FigParams[\"line_w\"],\n",
    "            FigParams[\"colors_maps\"],\n",
    "            FigParams[\"colors_names\"],\n",
    "            FigParams[\"saveFigPath\"],)\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "cm_data = [[0.2422, 0.1504, 0.6603],\n",
    "[0.2444, 0.1534, 0.6728],\n",
    "[0.2464, 0.1569, 0.6847],\n",
    "[0.2484, 0.1607, 0.6961],\n",
    "[0.2503, 0.1648, 0.7071],\n",
    "[0.2522, 0.1689, 0.7179],\n",
    "[0.254, 0.1732, 0.7286],\n",
    "[0.2558, 0.1773, 0.7393],\n",
    "[0.2576, 0.1814, 0.7501],\n",
    "[0.2594, 0.1854, 0.761],\n",
    "[0.2611, 0.1893, 0.7719],\n",
    "[0.2628, 0.1932, 0.7828],\n",
    "[0.2645, 0.1972, 0.7937],\n",
    "[0.2661, 0.2011, 0.8043],\n",
    "[0.2676, 0.2052, 0.8148],\n",
    "[0.2691, 0.2094, 0.8249],\n",
    "[0.2704, 0.2138, 0.8346],\n",
    "[0.2717, 0.2184, 0.8439],\n",
    "[0.2729, 0.2231, 0.8528],\n",
    "[0.274, 0.228, 0.8612],\n",
    "[0.2749, 0.233, 0.8692],\n",
    "[0.2758, 0.2382, 0.8767],\n",
    "[0.2766, 0.2435, 0.884],\n",
    "[0.2774, 0.2489, 0.8908],\n",
    "[0.2781, 0.2543, 0.8973],\n",
    "[0.2788, 0.2598, 0.9035],\n",
    "[0.2794, 0.2653, 0.9094],\n",
    "[0.2798, 0.2708, 0.915],\n",
    "[0.2802, 0.2764, 0.9204],\n",
    "[0.2806, 0.2819, 0.9255],\n",
    "[0.2809, 0.2875, 0.9305],\n",
    "[0.2811, 0.293, 0.9352],\n",
    "[0.2813, 0.2985, 0.9397],\n",
    "[0.2814, 0.304, 0.9441],\n",
    "[0.2814, 0.3095, 0.9483],\n",
    "[0.2813, 0.315, 0.9524],\n",
    "[0.2811, 0.3204, 0.9563],\n",
    "[0.2809, 0.3259, 0.96],\n",
    "[0.2807, 0.3313, 0.9636],\n",
    "[0.2803, 0.3367, 0.967],\n",
    "[0.2798, 0.3421, 0.9702],\n",
    "[0.2791, 0.3475, 0.9733],\n",
    "[0.2784, 0.3529, 0.9763],\n",
    "[0.2776, 0.3583, 0.9791],\n",
    "[0.2766, 0.3638, 0.9817],\n",
    "[0.2754, 0.3693, 0.984],\n",
    "[0.2741, 0.3748, 0.9862],\n",
    "[0.2726, 0.3804, 0.9881],\n",
    "[0.271, 0.386, 0.9898],\n",
    "[0.2691, 0.3916, 0.9912],\n",
    "[0.267, 0.3973, 0.9924],\n",
    "[0.2647, 0.403, 0.9935],\n",
    "[0.2621, 0.4088, 0.9946],\n",
    "[0.2591, 0.4145, 0.9955],\n",
    "[0.2556, 0.4203, 0.9965],\n",
    "[0.2517, 0.4261, 0.9974],\n",
    "[0.2473, 0.4319, 0.9983],\n",
    "[0.2424, 0.4378, 0.9991],\n",
    "[0.2369, 0.4437, 0.9996],\n",
    "[0.2311, 0.4497, 0.9995],\n",
    "[0.225, 0.4559, 0.9985],\n",
    "[0.2189, 0.462, 0.9968],\n",
    "[0.2128, 0.4682, 0.9948],\n",
    "[0.2066, 0.4743, 0.9926],\n",
    "[0.2006, 0.4803, 0.9906],\n",
    "[0.195, 0.4861, 0.9887],\n",
    "[0.1903, 0.4919, 0.9867],\n",
    "[0.1869, 0.4975, 0.9844],\n",
    "[0.1847, 0.503, 0.9819],\n",
    "[0.1831, 0.5084, 0.9793],\n",
    "[0.1818, 0.5138, 0.9766],\n",
    "[0.1806, 0.5191, 0.9738],\n",
    "[0.1795, 0.5244, 0.9709],\n",
    "[0.1785, 0.5296, 0.9677],\n",
    "[0.1778, 0.5349, 0.9641],\n",
    "[0.1773, 0.5401, 0.9602],\n",
    "[0.1768, 0.5452, 0.956],\n",
    "[0.1764, 0.5504, 0.9516],\n",
    "[0.1755, 0.5554, 0.9473],\n",
    "[0.174, 0.5605, 0.9432],\n",
    "[0.1716, 0.5655, 0.9393],\n",
    "[0.1686, 0.5705, 0.9357],\n",
    "[0.1649, 0.5755, 0.9323],\n",
    "[0.161, 0.5805, 0.9289],\n",
    "[0.1573, 0.5854, 0.9254],\n",
    "[0.154, 0.5902, 0.9218],\n",
    "[0.1513, 0.595, 0.9182],\n",
    "[0.1492, 0.5997, 0.9147],\n",
    "[0.1475, 0.6043, 0.9113],\n",
    "[0.1461, 0.6089, 0.908],\n",
    "[0.1446, 0.6135, 0.905],\n",
    "[0.1429, 0.618, 0.9022],\n",
    "[0.1408, 0.6226, 0.8998],\n",
    "[0.1383, 0.6272, 0.8975],\n",
    "[0.1354, 0.6317, 0.8953],\n",
    "[0.1321, 0.6363, 0.8932],\n",
    "[0.1288, 0.6408, 0.891],\n",
    "[0.1253, 0.6453, 0.8887],\n",
    "[0.1219, 0.6497, 0.8862],\n",
    "[0.1185, 0.6541, 0.8834],\n",
    "[0.1152, 0.6584, 0.8804],\n",
    "[0.1119, 0.6627, 0.877],\n",
    "[0.1085, 0.6669, 0.8734],\n",
    "[0.1048, 0.671, 0.8695],\n",
    "[0.1009, 0.675, 0.8653],\n",
    "[0.0964, 0.6789, 0.8609],\n",
    "[0.0914, 0.6828, 0.8562],\n",
    "[0.0855, 0.6865, 0.8513],\n",
    "[0.0789, 0.6902, 0.8462],\n",
    "[0.0713, 0.6938, 0.8409],\n",
    "[0.0628, 0.6972, 0.8355],\n",
    "[0.0535, 0.7006, 0.8299],\n",
    "[0.0433, 0.7039, 0.8242],\n",
    "[0.0328, 0.7071, 0.8183],\n",
    "[0.0234, 0.7103, 0.8124],\n",
    "[0.0155, 0.7133, 0.8064],\n",
    "[0.0091, 0.7163, 0.8003],\n",
    "[0.0046, 0.7192, 0.7941],\n",
    "[0.0019, 0.722, 0.7878],\n",
    "[0.0009, 0.7248, 0.7815],\n",
    "[0.0018, 0.7275, 0.7752],\n",
    "[0.0046, 0.7301, 0.7688],\n",
    "[0.0094, 0.7327, 0.7623],\n",
    "[0.0162, 0.7352, 0.7558],\n",
    "[0.0253, 0.7376, 0.7492],\n",
    "[0.0369, 0.74, 0.7426],\n",
    "[0.0504, 0.7423, 0.7359],\n",
    "[0.0638, 0.7446, 0.7292],\n",
    "[0.077, 0.7468, 0.7224],\n",
    "[0.0899, 0.7489, 0.7156],\n",
    "[0.1023, 0.751, 0.7088],\n",
    "[0.1141, 0.7531, 0.7019],\n",
    "[0.1252, 0.7552, 0.695],\n",
    "[0.1354, 0.7572, 0.6881],\n",
    "[0.1448, 0.7593, 0.6812],\n",
    "[0.1532, 0.7614, 0.6741],\n",
    "[0.1609, 0.7635, 0.6671],\n",
    "[0.1678, 0.7656, 0.6599],\n",
    "[0.1741, 0.7678, 0.6527],\n",
    "[0.1799, 0.7699, 0.6454],\n",
    "[0.1853, 0.7721, 0.6379],\n",
    "[0.1905, 0.7743, 0.6303],\n",
    "[0.1954, 0.7765, 0.6225],\n",
    "[0.2003, 0.7787, 0.6146],\n",
    "[0.2061, 0.7808, 0.6065],\n",
    "[0.2118, 0.7828, 0.5983],\n",
    "[0.2178, 0.7849, 0.5899],\n",
    "[0.2244, 0.7869, 0.5813],\n",
    "[0.2318, 0.7887, 0.5725],\n",
    "[0.2401, 0.7905, 0.5636],\n",
    "[0.2491, 0.7922, 0.5546],\n",
    "[0.2589, 0.7937, 0.5454],\n",
    "[0.2695, 0.7951, 0.536],\n",
    "[0.2809, 0.7964, 0.5266],\n",
    "[0.2929, 0.7975, 0.517],\n",
    "[0.3052, 0.7985, 0.5074],\n",
    "[0.3176, 0.7994, 0.4975],\n",
    "[0.3301, 0.8002, 0.4876],\n",
    "[0.3424, 0.8009, 0.4774],\n",
    "[0.3548, 0.8016, 0.4669],\n",
    "[0.3671, 0.8021, 0.4563],\n",
    "[0.3795, 0.8026, 0.4454],\n",
    "[0.3921, 0.8029, 0.4344],\n",
    "[0.405, 0.8031, 0.4233],\n",
    "[0.4184, 0.803, 0.4122],\n",
    "[0.4322, 0.8028, 0.4013],\n",
    "[0.4463, 0.8024, 0.3904],\n",
    "[0.4608, 0.8018, 0.3797],\n",
    "[0.4753, 0.8011, 0.3691],\n",
    "[0.4899, 0.8002, 0.3586],\n",
    "[0.5044, 0.7993, 0.348],\n",
    "[0.5187, 0.7982, 0.3374],\n",
    "[0.5329, 0.797, 0.3267],\n",
    "[0.547, 0.7957, 0.3159],\n",
    "[0.5609, 0.7943, 0.305],\n",
    "[0.5748, 0.7929, 0.2941],\n",
    "[0.5886, 0.7913, 0.2833],\n",
    "[0.6024, 0.7896, 0.2726],\n",
    "[0.6161, 0.7878, 0.2622],\n",
    "[0.6297, 0.7859, 0.2521],\n",
    "[0.6433, 0.7839, 0.2423],\n",
    "[0.6567, 0.7818, 0.2329],\n",
    "[0.6701, 0.7796, 0.2239],\n",
    "[0.6833, 0.7773, 0.2155],\n",
    "[0.6963, 0.775, 0.2075],\n",
    "[0.7091, 0.7727, 0.1998],\n",
    "[0.7218, 0.7703, 0.1924],\n",
    "[0.7344, 0.7679, 0.1852],\n",
    "[0.7468, 0.7654, 0.1782],\n",
    "[0.759, 0.7629, 0.1717],\n",
    "[0.771, 0.7604, 0.1658],\n",
    "[0.7829, 0.7579, 0.1608],\n",
    "[0.7945, 0.7554, 0.157],\n",
    "[0.806, 0.7529, 0.1546],\n",
    "[0.8172, 0.7505, 0.1535],\n",
    "[0.8281, 0.7481, 0.1536],\n",
    "[0.8389, 0.7457, 0.1546],\n",
    "[0.8495, 0.7435, 0.1564],\n",
    "[0.86, 0.7413, 0.1587],\n",
    "[0.8703, 0.7392, 0.1615],\n",
    "[0.8804, 0.7372, 0.165],\n",
    "[0.8903, 0.7353, 0.1695],\n",
    "[0.9, 0.7336, 0.1749],\n",
    "[0.9093, 0.7321, 0.1815],\n",
    "[0.9184, 0.7308, 0.189],\n",
    "[0.9272, 0.7298, 0.1973],\n",
    "[0.9357, 0.729, 0.2061],\n",
    "[0.944, 0.7285, 0.2151],\n",
    "[0.9523, 0.7284, 0.2237],\n",
    "[0.9606, 0.7285, 0.2312],\n",
    "[0.9689, 0.7292, 0.2373],\n",
    "[0.977, 0.7304, 0.2418],\n",
    "[0.9842, 0.733, 0.2446],\n",
    "[0.99, 0.7365, 0.2429],\n",
    "[0.9946, 0.7407, 0.2394],\n",
    "[0.9966, 0.7458, 0.2351],\n",
    "[0.9971, 0.7513, 0.2309],\n",
    "[0.9972, 0.7569, 0.2267],\n",
    "[0.9971, 0.7626, 0.2224],\n",
    "[0.9969, 0.7683, 0.2181],\n",
    "[0.9966, 0.774, 0.2138],\n",
    "[0.9962, 0.7798, 0.2095],\n",
    "[0.9957, 0.7856, 0.2053],\n",
    "[0.9949, 0.7915, 0.2012],\n",
    "[0.9938, 0.7974, 0.1974],\n",
    "[0.9923, 0.8034, 0.1939],\n",
    "[0.9906, 0.8095, 0.1906],\n",
    "[0.9885, 0.8156, 0.1875],\n",
    "[0.9861, 0.8218, 0.1846],\n",
    "[0.9835, 0.828, 0.1817],\n",
    "[0.9807, 0.8342, 0.1787],\n",
    "[0.9778, 0.8404, 0.1757],\n",
    "[0.9748, 0.8467, 0.1726],\n",
    "[0.972, 0.8529, 0.1695],\n",
    "[0.9694, 0.8591, 0.1665],\n",
    "[0.9671, 0.8654, 0.1636],\n",
    "[0.9651, 0.8716, 0.1608],\n",
    "[0.9634, 0.8778, 0.1582],\n",
    "[0.9619, 0.884, 0.1557],\n",
    "[0.9608, 0.8902, 0.1532],\n",
    "[0.9601, 0.8963, 0.1507],\n",
    "[0.9596, 0.9023, 0.148],\n",
    "[0.9595, 0.9084, 0.145],\n",
    "[0.9597, 0.9143, 0.1418],\n",
    "[0.9601, 0.9203, 0.1382],\n",
    "[0.9608, 0.9262, 0.1344],\n",
    "[0.9618, 0.932, 0.1304],\n",
    "[0.9629, 0.9379, 0.1261],\n",
    "[0.9642, 0.9437, 0.1216],\n",
    "[0.9657, 0.9494, 0.1168],\n",
    "[0.9674, 0.9552, 0.1116],\n",
    "[0.9692, 0.9609, 0.1061],\n",
    "[0.9711, 0.9667, 0.1001],\n",
    "[0.973, 0.9724, 0.0938],\n",
    "[0.9749, 0.9782, 0.0872],\n",
    "[0.9769, 0.9839, 0.0805]]\n",
    "\n",
    "parula_map = LinearSegmentedColormap.from_list('parula', cm_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eb27e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "figS, f_font1, f_font2, line_w, cm, cn, sp = packParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meanfire']\n",
      "(61, 61, 5, 6, 3, 500, 9)\n",
      "(61, 61, 5, 6, 3, 500, 9)\n"
     ]
    }
   ],
   "source": [
    "# open the file in read mode\n",
    "with h5py.File('/Users/glengyel/Projects_data/Neural Predictions/MF_all_full.mat', 'r') as file:\n",
    "    # list all the keys in the root group\n",
    "    print(list(file.keys()))\n",
    "    # access a dataset\n",
    "    textures = file['meanfire']\n",
    "    print(textures.shape)\n",
    "    textures = np.array(textures)\n",
    "    print(textures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ad226f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 61, 5, 5, 3, 500, 9)\n"
     ]
    }
   ],
   "source": [
    "textures = textures[:,:,:,1:,:,:,:]\n",
    "print(textures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e82471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337500 153199.0\n"
     ]
    }
   ],
   "source": [
    "def morans_i(image):\n",
    "    # Get the mean of the image\n",
    "    mean_value = np.mean(image)\n",
    "    # Calculate deviations from the mean\n",
    "    deviation = image - mean_value\n",
    "    # Get the number of rows and columns\n",
    "    rows, cols = image.shape\n",
    "    # Define the weight matrix (adjacency: 4-neighborhood)\n",
    "    w = np.zeros((rows, cols, rows, cols))\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if i > 0:  # Up\n",
    "                w[i, j, i-1, j] = 1\n",
    "            if i < rows - 1:  # Down\n",
    "                w[i, j, i+1, j] = 1\n",
    "            if j > 0:  # Left\n",
    "                w[i, j, i, j-1] = 1\n",
    "            if j < cols - 1:  # Right\n",
    "                w[i, j, i, j+1] = 1\n",
    "\n",
    "    # Flatten arrays for calculation\n",
    "    flat_deviation = deviation.flatten()\n",
    "    flat_w = w.reshape(rows * cols, rows * cols)\n",
    "    \n",
    "    # Moran's I numerator\n",
    "    numerator = np.dot(flat_deviation.T, np.dot(flat_w, flat_deviation))\n",
    "    # Moran's I denominator\n",
    "    denominator = np.sum(flat_deviation ** 2)\n",
    "    # Moran's I calculation\n",
    "    morans_i_value = (rows * cols / np.sum(flat_w)) * (numerator / denominator)\n",
    "    \n",
    "    return morans_i_value\n",
    "\n",
    "n_ix = np.zeros((textures.shape[2],textures.shape[3],textures.shape[4],textures.shape[5],textures.shape[6]))\n",
    "Moran = np.zeros((textures.shape[2],textures.shape[3],textures.shape[4],textures.shape[5],textures.shape[6]))\n",
    "for cs in range(textures.shape[2]):\n",
    "    for ss in range(textures.shape[3]):\n",
    "        for n in range(textures.shape[4]):\n",
    "            for p in range(textures.shape[5]):\n",
    "                for e in range(textures.shape[6]):\n",
    "                    Moran[cs,ss,n,p,e]= morans_i(textures[:,:,cs,ss,n,p,e])\n",
    "                    if Moran[cs,ss,n,p,e]>0.95:\n",
    "                        n_ix[cs,ss,n,p,e] = 1\n",
    "print((textures.shape[2]*textures.shape[3]*textures.shape[4]*textures.shape[5]*textures.shape[6]), np.sum(n_ix))\n",
    "# for ix in np.where(np.array(Moran[n_ix])<0.951)[0]:\n",
    "#     print(\"Moran's I:\", Moran[n_ix[ix]])\n",
    "#     plt.imshow(textures[:,:,n_ix[ix]], cmap='gray')\n",
    "#     plt.show()\n",
    "# for i in range(500):\n",
    "#     if Moran[i]>0.95:\n",
    "#         print(\"Moran's I:\", Moran[i])\n",
    "#         plt.imshow(textures[:,:, i], cmap='gray')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41e2b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Moran.npy\", Moran)\n",
    "np.save(\"n_ix.npy\", n_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f1a53",
   "metadata": {},
   "source": [
    "## Correlating all surfaces with each other and picking the ones that are not noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff82db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Moran = np.load(\"Moran.npy\")\n",
    "n_ix = np.load(\"n_ix.npy\")\n",
    "flat_textures = textures.reshape(textures.shape[0], textures.shape[1], -1)\n",
    "flat_n_ix = n_ix.reshape(-1).astype(bool)\n",
    "flat_moran = Moran.reshape(-1)\n",
    "original_n_ix = np.unravel_index(range(len(flat_n_ix)), n_ix.shape)\n",
    "flat_textures = flat_textures[:,:,flat_n_ix]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f57f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_correlations_with_np(batch_textures, all_textures):\n",
    "    \"\"\"\n",
    "    Computes the correlation between a batch of textures and all textures using np.corrcoef.\n",
    "    \n",
    "    Parameters:\n",
    "    - batch_textures: A batch of 3D textures (height x width x batch_size)\n",
    "    - all_textures: All textures in the dataset (height x width x total_num_textures)\n",
    "    \n",
    "    Returns:\n",
    "    - correlations: A 2D array of shape (batch_size, total_num_textures), where each row\n",
    "                    contains the correlations of one texture in the batch with all textures.\n",
    "    \"\"\"\n",
    "    # Flatten the textures for correlation computation\n",
    "    batch_size = batch_textures.shape[2]\n",
    "    total_textures = all_textures.shape[2]\n",
    "    \n",
    "    # Flatten both batch and all textures\n",
    "    batch_textures_flat = batch_textures.reshape(-1, batch_size)  # Shape: (height*width, batch_size)\n",
    "    all_textures_flat = all_textures.reshape(-1, total_textures)  # Shape: (height*width, total_num_textures)\n",
    "    \n",
    "    # Concatenate the batch with the all textures\n",
    "    combined = np.concatenate((batch_textures_flat, all_textures_flat), axis=1)\n",
    "\n",
    "    # Compute the full correlation matrix\n",
    "    corr_matrix = np.corrcoef(combined, rowvar=False)\n",
    "\n",
    "    # Extract the relevant part of the correlation matrix\n",
    "    # First `batch_size` rows correspond to batch textures, and the rest correspond to all_textures\n",
    "    correlations = corr_matrix[:batch_size, batch_size:]\n",
    "\n",
    "    return correlations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6d03d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# Assuming flat_textures is a 3D array of shape (height, width, num_textures)\n",
    "batch_size = 10\n",
    "\n",
    "# Let's say the first 10 textures form your small batch\n",
    "batch_textures = flat_textures[:, :, :batch_size]\n",
    "\n",
    "# Process in chunks if necessary\n",
    "total_textures = flat_textures.shape[2]\n",
    "correlations_all_batches = []\n",
    "\n",
    "#for start in range(0, total_textures, chunk_size):\n",
    "    #print(f\"Processing chunk {start} to {min(start + chunk_size, total_textures)}\")\n",
    "    #end = min(start + chunk_size, total_textures)\n",
    "    \n",
    "    # Extract a chunk of all textures\n",
    "    #chunk_textures = flat_textures[:, :, start:end]\n",
    "    \n",
    "    # Compute correlations between the batch and the current chunk of textures\n",
    "    #correlations = compute_batch_correlations_with_np(batch_textures, chunk_textures)\n",
    "    \n",
    "    # Store the results\n",
    "    #correlations_all_batches.append(correlations)\n",
    "\n",
    "corr = compute_batch_correlations_with_np(batch_textures, flat_textures)\n",
    "\n",
    "plt.imshow(corr, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a correlation threshold above which images will be considered as 'correlating a lot'\n",
    "corr_threshold = 0.95  # You can adjust this threshold as needed\n",
    "# Initialize a list to keep track of indices of images to retain\n",
    "keep_indices = []\n",
    "# Initialize a boolean array to mark which images have already been checked\n",
    "checked = np.zeros(corr.shape[0], dtype=bool)\n",
    "# Iterate over the correlation matrix to filter out highly correlated images\n",
    "for i in range(corr.shape[0]):\n",
    "    if not checked[i]:  # Only proceed if the image hasn't already been checked\n",
    "        # Mark the current image as checked\n",
    "        checked[i] = True\n",
    "        # Find indices of images that are highly correlated with the current image\n",
    "        high_corr_indices = np.where(corr[i] > corr_threshold)[0]\n",
    "        # Mark all these correlated images as checked\n",
    "        checked[high_corr_indices] = True\n",
    "        # Mark all highly correlated images as False (discard them)\n",
    "        keep_indices.append(high_corr_indices[np.argmax(Moran[high_corr_indices])])\n",
    "\n",
    "# Use the mask to filter the images\n",
    "filtered_textures = textures[:, :, keep_indices]\n",
    "\n",
    "print(f\"Original number of images: {textures.shape[2]}\")\n",
    "print(f\"Number of images after filtering: {filtered_textures.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11b999e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7v/b8fs48v95fxb7gdt_3bqj7q00000gp/T/ipykernel_74632/6611308.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mIX\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Compare with remaining textures in the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_textures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_textures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcorr_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcorrcoef\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mcorrcoef\u001b[0;34m(x, y, rowvar, bias, ddof, dtype)\u001b[0m\n\u001b[1;32m   2632\u001b[0m         warnings.warn('bias and ddof have no effect and are deprecated',\n\u001b[1;32m   2633\u001b[0m                       DeprecationWarning, stacklevel=3)\n\u001b[0;32m-> 2634\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrowvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2635\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcov\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mcov\u001b[0;34m(m, y, rowvar, bias, ddof, fweights, aweights, dtype)\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0mfact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2487\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2488\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m         \u001b[0mX_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corr_threshold = 0.95  # You can adjust this threshold as needed\n",
    "keep_indices = [[] for i in range(flat_textures.shape[2])]\n",
    "keep_textures = []\n",
    "IX = list(range(flat_textures.shape[2]))\n",
    "j = 0\n",
    "while IX:  # Keep looping until there are no more textures to check\n",
    "    print(j)\n",
    "    i1 = IX.pop(0)  # Take the first texture from the list\n",
    "    above_thr = [flat_textures[:, :, i1]]  # Include the current texture itself\n",
    "    to_remove = []  # Track indices to remove after finding correlated ones\n",
    "\n",
    "    for i2 in IX:  # Compare with remaining textures in the list\n",
    "        corr = np.corrcoef(flat_textures[:, :, i1].flatten(), flat_textures[:, :, i2].flatten())[0, 1]\n",
    "        \n",
    "        if corr > corr_threshold:\n",
    "            above_thr.append(flat_textures[:, :, i2])\n",
    "            keep_indices[i1].append([original_n_ix[i][i2] for i in range(len(original_n_ix))])\n",
    "            to_remove.append(i2)  # Collect indices to be removed after the inner loop\n",
    "    \n",
    "    if above_thr:  # Only compute mean if textures were found\n",
    "        keep_textures.append(np.mean(np.array(above_thr), axis=0))\n",
    "    \n",
    "    # Remove indices after the loop to avoid skipping during iteration\n",
    "    IX = [ix for ix in IX if ix not in to_remove]\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60ab80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a122d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ecc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb99475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18016a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, k1, k2, out1, out2):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, out1, k1, stride=2, padding=1), # Adjust input channels as needed\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out1, out2, k2, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(out2, out1, k2, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(out1, 1, k1, stride=2, padding=1, output_padding=1), # Adjust output channels as needed\n",
    "            nn.Sigmoid() # Assuming input textures are normalized [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e46cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_textures = torch.tensor(textures).float()\n",
    "dataset = TensorDataset(tensor_textures)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f30815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (Olivetti faces as an example)\n",
    "data = fetch_olivetti_faces()\n",
    "faces = data.images\n",
    "faces = torch.tensor(faces).unsqueeze(1).float()  # Add channel dimension for grayscale images\n",
    "dataset = TensorDataset(faces)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "302bb894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model(k1, k2, out1, out2, nE, dataloader, device):\n",
    "    model = ConvAutoencoder(k1, k2, out1, out2).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = nE # Adjust as needed\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in dataloader:\n",
    "            inputs, = data\n",
    "            #print(inputs.shape)\n",
    "            optimizer.zero_grad()\n",
    "            _, decoded = model(inputs)\n",
    "            #print(decoded.shape)\n",
    "            loss = criterion(decoded, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    return model\n",
    "\n",
    "def extract_features_kmean(model, dataloader, nC, rS, textures):\n",
    "    # Use the trained encoder to extract features\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, = data\n",
    "            encoded, _ = model(inputs)\n",
    "            #print(encoded.shape)\n",
    "            features.append(encoded.view(encoded.size(0), -1).cpu().numpy())\n",
    "    sh = encoded.shape\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    #print(features.shape)\n",
    "    # Perform k-means clustering\n",
    "    kmeans = KMeans(n_clusters=nC, random_state=rS).fit(features)\n",
    "    # Calculate the distance of each feature to each cluster center\n",
    "    distances = cdist(features, kmeans.cluster_centers_, 'euclidean')\n",
    "    # Find the index of the closest sample to each cluster center\n",
    "    closest_indices = np.argmin(distances, axis=0)\n",
    "    # Use these indices to select the representative textures from the original dataset\n",
    "    representative_textures = [textures[i] for i in closest_indices]\n",
    "    categories = kmeans.predict(features)\n",
    "    return kmeans, representative_textures, sh, distances, categories\n",
    "\n",
    "def extract_features_hc(model, dataloader, textures, dT):\n",
    "    # Use the trained encoder to extract features\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, = data\n",
    "            encoded, _ = model(inputs)\n",
    "            #print(encoded.shape)\n",
    "            features.append(encoded.view(encoded.size(0), -1).cpu().numpy())\n",
    "    sh = encoded.shape\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    #print(features.shape)\n",
    "    # Perform hierarchical clustering clustering\n",
    "    clustering = AgglomerativeClustering(distance_threshold=dT, n_clusters=None).fit(features)\n",
    "    categories = clustering.labels_\n",
    "    cluster_centers = np.array([features[categories == i].mean(axis=0) for i in np.unique(categories)])\n",
    "    # Calculate the distance of each feature to each cluster center\n",
    "    distances = cdist(features, cluster_centers, 'euclidean')\n",
    "    # Find the index of the closest sample to each cluster center\n",
    "    closest_indices = np.argmin(distances, axis=0)\n",
    "    # Use these indices to select the representative textures from the original dataset\n",
    "    representative_textures = [textures[i] for i in closest_indices]\n",
    "    return clustering, representative_textures, sh, distances, categories, cluster_centers\n",
    "\n",
    "def extract_features_dbsc(model, dataloader, textures, eps, min_samples):\n",
    "    # Use the trained encoder to extract features\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, = data\n",
    "            encoded, _ = model(inputs)\n",
    "            #print(encoded.shape)\n",
    "            features.append(encoded.view(encoded.size(0), -1).cpu().numpy())\n",
    "    sh = encoded.shape\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    #print(features.shape)\n",
    "    # Perform DBSCAN clustering\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan.fit(features)\n",
    "    # DBSCAN labels for the dataset\n",
    "    categories = dbscan.labels_\n",
    "    # Number of clusters\n",
    "    n_clusters_ = len(set(categories)) - (1 if -1 in categories else 0) \n",
    "    cluster_centers = np.array([features[categories == i].mean(axis=0) for i in range(n_clusters_)])\n",
    "    # Calculate the distance of each feature to each cluster center\n",
    "    distances = cdist(features, cluster_centers, 'euclidean')\n",
    "    # Find the index of the closest sample to each cluster center\n",
    "    closest_indices = np.argmin(distances, axis=0)\n",
    "    # Use these indices to select the representative textures from the original dataset\n",
    "    representative_textures = [textures[i] for i in closest_indices]\n",
    "    return dbscan, representative_textures, sh, distances, categories, cluster_centers\n",
    "\n",
    "def generate_textures(kmeans, model, sh, device, cc=None):\n",
    "    if cc is None:\n",
    "        cc = kmeans.cluster_centers_\n",
    "    # Assuming `model` is your trained model and it has a decoder that can take latent space representations\n",
    "    model.eval()\n",
    "    generated_textures = []\n",
    "    with torch.no_grad():\n",
    "        for center in cc:\n",
    "            # Reshape and convert the cluster center to a tensor\n",
    "            #print(center.shape)\n",
    "            center_tensor = torch.tensor(center, dtype=torch.float32).view(sh[1], sh[2], sh[3])  # Adjust shape as necessary\n",
    "            center_tensor = center_tensor.to(device)  # If using GPU\n",
    "            # Decode the center tensor to get the generated texture\n",
    "            generated_texture = model.decoder(center_tensor)\n",
    "            T = np.squeeze(generated_texture.cpu().numpy())\n",
    "            #plt.imshow(T, cmap='gray')\n",
    "            #plt.show()\n",
    "            generated_textures.append(T)\n",
    "    return generated_textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef462b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `features` is the array of encoded features from your dataset\n",
    "# and `kmeans` is your trained KMeans model\n",
    "\n",
    "k1, k2, out1, out2 = 3, 3, 16, 8\n",
    "m = train_model(k1, k2, out1, out2, 50, dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ed36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [60,120]:\n",
    "    k,texts,sh,dist,c,cc = extract_features_hc(m, dataloader, faces, i)\n",
    "    T = generate_textures(k, m, sh, device,cc)\n",
    "    np.unique(c)\n",
    "    n = np.array([sum(c==j) for j in np.unique(c)])\n",
    "    plt.bar(range(len(np.unique(c))), n, color='blue', alpha=0.7)\n",
    "    plt.show()\n",
    "    print(np.where(n>100))\n",
    "    _, ax = plt.subplots(2, 5, figsize=(figS*8, figS*4))\n",
    "    #ix = [3,5,7,8,9,14,15,17,18,20,22,23,24,26,27,28,30,32,33,34]\n",
    "    corrp = np.zeros(len(T))\n",
    "    ax = ax.ravel()\n",
    "    for p in range(len(T)):\n",
    "        t = T[p]\n",
    "\n",
    "        t = np.flip(t.T, axis=0)\n",
    "        ax[p].imshow(t,cmap=parula_map)\n",
    "        ax[p].spines[\"top\"].set_visible(False)\n",
    "        ax[p].spines[\"right\"].set_visible(False)\n",
    "\n",
    "        ax[p].grid(False)\n",
    "\n",
    "    #     t1,t2 = t[:,:30],np.flip(t[:,31:], axis=1)\n",
    "    #     corrp[p] = np.corrcoef(t1.flatten(), t2.flatten())[0,1]\n",
    "    #     ax[p].set_title(\"Cluster \"+str(p+1)+\"-\"+str(np.round(corrp[p],2)))\n",
    "    # plt.tight_layout()\n",
    "    #plt.savefig(sp+\"firing_base_pred\"+str(mi)+\".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tk=[]\n",
    "D = []\n",
    "C = []\n",
    "for r in range(5):\n",
    "    print(r)\n",
    "    k,texts,sh,dist,c = extract_features_kmean(m, dataloader, 36, r, faces)\n",
    "    Tk.append(generate_textures(k, m, sh, device))\n",
    "    D.append(dist)\n",
    "    C.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cba0f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    n = np.array([sum(C[i]==j) for j in np.unique(C[i])])\n",
    "    plt.bar(range(len(np.unique(C[i]))), n, color='blue', alpha=0.7)\n",
    "    plt.show()\n",
    "    print(np.where(n>100))\n",
    "    _, ax = plt.subplots(6, 6, figsize=(figS*3, figS*3))\n",
    "    ax = ax.ravel()\n",
    "    corrp = np.zeros(len(Tk[i]))\n",
    "    for p in range(len(Tk[i])):\n",
    "        t = np.flip(Tk[i][p].T, axis=0)\n",
    "        ax[p].imshow(t,cmap=parula_map)\n",
    "        ax[p].spines[\"top\"].set_visible(False)\n",
    "        ax[p].spines[\"right\"].set_visible(False)\n",
    "        #ax.spines[\"left\"].set_visible(False)\n",
    "        #ax.spines[\"bottom\"].set_visible(False)\n",
    "        #ax.spines[\"left\"].set_visible(False)\n",
    "        ax[p].grid(False)\n",
    "        # for label in (ax.get_xticklabels()+ax.get_yticklabels()):\n",
    "        #     label.set_fontname(f_font1[\"fontname\"])\n",
    "        #     label.set_fontsize(35)\n",
    "        #ax.xaxis.set_ticks([-4,0,4])\n",
    "        #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "        #ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "        #ax.yaxis.set_ticks([-4,0,4])\n",
    "        #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "        #ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "        #ax.set_xlim(-4.1, 4.1)\n",
    "        #ax.set_ylim(-4.1, 4.1)\n",
    "        #plt.tick_params(top=False, bottom=False, left=False, right=False,\n",
    "        #                labelleft=False, labelbottom=False)\n",
    "        # t1,t2 = t[:,:30],np.flip(t[:,31:], axis=1)\n",
    "        # corrp[p] = np.corrcoef(t1.flatten(), t2.flatten())[0,1]\n",
    "        # ax[p].set_title(\"Cluster \"+str(p+1)+\"-\"+str(np.round(corrp[p],2)))\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(sp+\"firing_base_pred\"+str(mi)+\".pdf\")\n",
    "    plt.show()\n",
    "\n",
    "    ix = np.argsort(corrp)[:15]\n",
    "    _, ax = plt.subplots(1, 15, figsize=(figS*15, figS))\n",
    "    ax = ax.ravel()\n",
    "    for p in range(len(ix)):\n",
    "        t = np.flip(Tk[i][ix[p]].T, axis=0)\n",
    "        ax[p].imshow(t,cmap=parula_map)\n",
    "        ax[p].spines[\"top\"].set_visible(False)\n",
    "        ax[p].spines[\"right\"].set_visible(False)\n",
    "\n",
    "        ax[p].grid(False)\n",
    "        ax[p].xaxis.set_ticks([])\n",
    "        #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "        #ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "        ax[p].yaxis.set_ticks([])\n",
    "        #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "        #ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "        #above = 1 if n[p]>100 else 0\n",
    "        #ax[p].set_title(\"DS: \"+str(np.round(1-corrp[ix[p]],2)), **f_font1)\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig(sp+\"clusters_km\"+str(i)+\".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51870a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset of face images\n",
    "data = fetch_olivetti_faces()\n",
    "faces = data.images  # shape (400, 64, 64) -> 400 images of 64x64 pixels\n",
    "\n",
    "# Step 1: Compute the mean image and subtract it from all images\n",
    "mean_face = np.mean(faces, axis=0)\n",
    "centered_faces = faces - mean_face\n",
    "\n",
    "# Step 2: Perform 2D PCA (compute the covariance matrix for the rows and columns)\n",
    "cov_rows = np.cov(centered_faces.reshape(-1, 64).T)  # Covariance of rows\n",
    "cov_cols = np.cov(centered_faces.reshape(64, -1))    # Covariance of columns\n",
    "\n",
    "# Step 3: Perform eigen-decomposition on the row and column covariance matrices\n",
    "eigenvalues_rows, eigenvectors_rows = np.linalg.eigh(cov_rows)\n",
    "eigenvalues_cols, eigenvectors_cols = np.linalg.eigh(cov_cols)\n",
    "\n",
    "# Step 4: Select the top k eigenvectors to project the data (dimensionality reduction)\n",
    "k = 30  # Number of principal components\n",
    "projected_faces = np.zeros((400, 64, 64))\n",
    "\n",
    "# Project each image onto the top k eigenvectors of rows and columns\n",
    "for i in range(400):\n",
    "    temp_face = centered_faces[i]\n",
    "    temp_face = eigenvectors_rows[:, -k:] @ (eigenvectors_rows[:, -k:].T @ temp_face)\n",
    "    projected_faces[i] = temp_face @ eigenvectors_cols[:, -k:] @ eigenvectors_cols[:, -k:].T\n",
    "\n",
    "# Step 5: Visualize original and projected faces\n",
    "def plot_faces(original, projected, h, w, n=5):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(n):\n",
    "        # Original image\n",
    "        plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(original[i].reshape((h, w)), cmap='gray')\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Projected image (2D PCA)\n",
    "        plt.subplot(2, n, i + n + 1)\n",
    "        plt.imshow(projected[i].reshape((h, w)), cmap='gray')\n",
    "        plt.title(\"Projected\")\n",
    "        plt.axis('off')\n",
    "\n",
    "plot_faces(faces, projected_faces, 64, 64)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62348a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset (Olivetti faces as an example)\n",
    "data = fetch_olivetti_faces()\n",
    "faces = data.images\n",
    "faces = torch.tensor(faces).unsqueeze(1).float()  # Add channel dimension for grayscale images\n",
    "\n",
    "# Define autoencoder architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder: compress image\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # downsample\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=2, padding=1),   # downsample\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Decoder: reconstruct image\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),  # Output should be in the range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Instantiate the model, define loss and optimizer\n",
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 50\n",
    "batch_size = 16\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, len(faces), batch_size):\n",
    "        batch = faces[i:i + batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{n_epochs}, Loss: {epoch_loss / len(faces)}')\n",
    "\n",
    "# Visualize some original and reconstructed images\n",
    "with torch.no_grad():\n",
    "    sample_faces = faces[:10]\n",
    "    reconstructed_faces = model(sample_faces).squeeze(1)\n",
    "\n",
    "def plot_faces(original, reconstructed):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(10):\n",
    "        # Original face\n",
    "        plt.subplot(2, 10, i + 1)\n",
    "        plt.imshow(original[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        # Reconstructed face\n",
    "        plt.subplot(2, 10, i + 11)\n",
    "        plt.imshow(reconstructed[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "plot_faces(faces[:10].squeeze(1).numpy(), reconstructed_faces.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fcc0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "textures = textures[n_ix]\n",
    "\n",
    "corr = np.corrcoef(textures.reshape(textures.shape[0]*textures.shape[1], textures.shape[2]).T)\n",
    "\n",
    "plt.imshow(corr, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab79cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Moran = Moran[n_ix]\n",
    "print(len(Moran))\n",
    "print(Moran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efad1864",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_sub_list = []\n",
    "final_ix = list(range(textures.shape[2]))\n",
    "for i in range(textures.shape[2]):\n",
    "\n",
    "    if i in final_ix:\n",
    "        sames_ix = list(np.where(corr[i,:]>0.95)[0])\n",
    "        if len(sames_ix)>1:\n",
    "            sames_m = Moran[sames_ix]\n",
    "            max_m_ix = np.argmax(sames_m)\n",
    "            final_set = set(final_ix)\n",
    "            same_set = set(sames_ix)\n",
    "            include_set = set(include_sub_list)\n",
    "            common_items = list(final_set & include_set)\n",
    "            if len(common_items)>0:\n",
    "                same_set = same_set - set(common_items)\n",
    "            final_ix = list(final_set - same_set)\n",
    "            final_ix.append(sames_ix[max_m_ix])\n",
    "            include_sub_list.append(sames_ix[max_m_ix])\n",
    "\n",
    "print(len(final_ix))\n",
    "# for ix in random.sample(final_ix, 10):\n",
    "#     plt.imshow(textures[:,:,ix], cmap='gray')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(20, 20, figsize=(figS*5, figS*5))\n",
    "ax=ax.flatten()\n",
    "corrp = np.zeros(len(final_ix))\n",
    "for p,pv in enumerate(final_ix):\n",
    "    t = textures[:,:,pv]\n",
    "    t = np.flip(t.T, axis=0)\n",
    "    ax[p].imshow(t,cmap=parula_map)\n",
    "    ax[p].spines[\"top\"].set_visible(False)\n",
    "    ax[p].spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax[p].grid(False)\n",
    "\n",
    "    t1,t2 = t[:,:30],np.flip(t[:,31:], axis=1)\n",
    "    corrp[p] = np.corrcoef(t1.flatten(), t2.flatten())[0,1]\n",
    "    ax[p].set_title(\"Cluster \"+str(p+1)+\"-\"+str(np.round(corrp[p],2)))\n",
    "plt.tight_layout()\n",
    "#plt.savefig(sp+\"firing_base_pred\"+str(mi)+\".pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a correlation threshold above which images will be considered as 'correlating a lot'\n",
    "corr_threshold = 0.95  # You can adjust this threshold as needed\n",
    "# Initialize a list to keep track of indices of images to retain\n",
    "keep_indices = []\n",
    "# Initialize a boolean array to mark which images have already been checked\n",
    "checked = np.zeros(corr.shape[0], dtype=bool)\n",
    "# Iterate over the correlation matrix to filter out highly correlated images\n",
    "for i in range(corr.shape[0]):\n",
    "    if not checked[i]:  # Only proceed if the image hasn't already been checked\n",
    "        # Mark the current image as checked\n",
    "        checked[i] = True\n",
    "        # Find indices of images that are highly correlated with the current image\n",
    "        high_corr_indices = np.where(corr[i] > corr_threshold)[0]\n",
    "        # Mark all these correlated images as checked\n",
    "        checked[high_corr_indices] = True\n",
    "        # Mark all highly correlated images as False (discard them)\n",
    "        keep_indices.append(high_corr_indices[np.argmax(Moran[high_corr_indices])])\n",
    "\n",
    "# Use the mask to filter the images\n",
    "filtered_textures = textures[:, :, keep_indices]\n",
    "\n",
    "print(f\"Original number of images: {textures.shape[2]}\")\n",
    "print(f\"Number of images after filtering: {filtered_textures.shape[2]}\")\n",
    "\n",
    "# The 'filtered_textures' array now contains only the non-correlating images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(15, 20, figsize=(figS*7, figS*5))\n",
    "ax=ax.flatten()\n",
    "corrp = np.zeros(filtered_textures.shape[2])\n",
    "for p in range(filtered_textures.shape[2]):\n",
    "    #print(p)\n",
    "    t = filtered_textures[:,:,p]\n",
    "    #print([np.corrcoef(t.flatten(), filtered_textures[:,:,i].flatten())[0,1] for i in range(filtered_textures.shape[2])])\n",
    "    t = np.flip(t.T, axis=0)\n",
    "    ax[p].imshow(t,cmap=parula_map)\n",
    "    ax[p].spines[\"top\"].set_visible(False)\n",
    "    ax[p].spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax[p].grid(False)\n",
    "\n",
    "    t1,t2 = t[:,:30],np.flip(t[:,31:], axis=1)\n",
    "    corrp[p] = np.corrcoef(t1.flatten(), t2.flatten())[0,1]\n",
    "    ax[p].set_title(\"Cluster \"+str(p+1)+\"-\"+str(np.round(corrp[p],2)))\n",
    "plt.tight_layout()\n",
    "#plt.savefig(sp+\"firing_base_pred\"+str(mi)+\".pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb214eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_ix = [1, 2, 3, 4,9,14,15,18,19,20,21,23,33,36,38,40,42,45,54,55,74,79,80,81,86,89,93,97,99,117,124,132,138,140,141,154,157,158,160,163,175,176,178,180,182,186,191,192,199, 201, 223,225,229,231,233,239,246,250,265,272,279,281,287,293]\n",
    "print(len(cluster_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "35599b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrp = np.zeros(len(cluster_ix))\n",
    "for p,pv in enumerate(cluster_ix):\n",
    "    t = filtered_textures[:,:,pv]\n",
    "    t = np.flip(t.T, axis=0)\n",
    "    t1,t2 = t[:,:30],np.flip(t[:,31:], axis=1)\n",
    "    corrp[p] = np.corrcoef(t1.flatten(), t2.flatten())[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186836e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ix[sorted_corr[67]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2bc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "64/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(4, 16, figsize=(figS*8, figS*2))\n",
    "ax=ax.flatten()\n",
    "tckix = [0, 30, 60]\n",
    "tckval = [\"-180\", \"0\", \"180\"]\n",
    "sorted_corr = np.argsort(corrp)\n",
    "for p,v in enumerate(sorted_corr):\n",
    "    t = np.flip(filtered_textures[:,:,cluster_ix[v]].T, axis=0)\n",
    "    ax[p].imshow(t,cmap=parula_map)\n",
    "    ax[p].spines[\"top\"].set_visible(False)\n",
    "    ax[p].spines[\"right\"].set_visible(False)\n",
    "    ax[p].grid(False)\n",
    "    ax[p].set_title(\"#\"+str(p+1)+\" - \"+str(np.round(corrp[v],2)),**f_font2)\n",
    "    if p%16==0:\n",
    "        ax[p].yaxis.set_ticks(tckix)\n",
    "        ax[p].yaxis.set_ticklabels(tckval, **f_font1)\n",
    "        if p>47:\n",
    "            ax[p].xaxis.set_ticks(tckix)\n",
    "            ax[p].xaxis.set_ticklabels(tckval, **f_font1)\n",
    "        else:\n",
    "            ax[p].xaxis.set_ticks([ ])\n",
    "    elif p>47:\n",
    "        ax[p].yaxis.set_ticks([ ])\n",
    "        ax[p].xaxis.set_ticks(tckix)\n",
    "        ax[p].xaxis.set_ticklabels(tckval, **f_font1)\n",
    "    else:\n",
    "        ax[p].xaxis.set_ticks([ ])\n",
    "        ax[p].yaxis.set_ticks([ ])\n",
    "    for labl in (ax[p].get_xticklabels()+ax[p].get_yticklabels()):\n",
    "        labl.set_fontname(f_font1[\"fontname\"])\n",
    "        labl.set_fontsize(f_font1[\"size\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(sp+\"firing_base_pred\"+str(mi)+\".pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344aa5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(8, 8, figsize=(figS*4, figS*4))\n",
    "ax=ax.flatten()\n",
    "tckix = [0, 30, 60]\n",
    "tckval = [\"-180\", \"0\", \"180\"]\n",
    "sorted_corr = np.argsort(corrp)\n",
    "for p,v in enumerate(sorted_corr):\n",
    "    t = np.flip(filtered_textures[:,:,cluster_ix[v]].T, axis=0)\n",
    "    ax[p].imshow(t,cmap=parula_map)\n",
    "    ax[p].spines[\"top\"].set_visible(False)\n",
    "    ax[p].spines[\"right\"].set_visible(False)\n",
    "    ax[p].grid(False)\n",
    "    ax[p].set_title(\"#\"+str(p+1)+\" - \"+str(np.round(corrp[v],2)),**f_font2)\n",
    "    if p%8==0:\n",
    "        ax[p].yaxis.set_ticks(tckix)\n",
    "        ax[p].yaxis.set_ticklabels(tckval, **f_font1)\n",
    "        if p>55:\n",
    "            ax[p].xaxis.set_ticks(tckix)\n",
    "            ax[p].xaxis.set_ticklabels(tckval, **f_font1)\n",
    "        else:\n",
    "            ax[p].xaxis.set_ticks([ ])\n",
    "    elif p>55:\n",
    "        ax[p].yaxis.set_ticks([ ])\n",
    "        ax[p].xaxis.set_ticks(tckix)\n",
    "        ax[p].xaxis.set_ticklabels(tckval, **f_font1)\n",
    "    else:\n",
    "        ax[p].xaxis.set_ticks([ ])\n",
    "        ax[p].yaxis.set_ticks([ ])\n",
    "    for labl in (ax[p].get_xticklabels()+ax[p].get_yticklabels()):\n",
    "        labl.set_fontname(f_font1[\"fontname\"])\n",
    "        labl.set_fontsize(f_font1[\"size\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(sp+\"firing_base_pred\"+str(mi)+\".pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for p,pv in enumerate(cluster_ix):\n",
    "    #print(p)\n",
    "    t = filtered_textures[:,:,pv]\n",
    "    #print([np.corrcoef(t.flatten(), filtered_textures[:,:,i].flatten())[0,1] for i in range(filtered_textures.shape[2])])\n",
    "    t = np.flip(t.T, axis=0)\n",
    "    ax[p].imshow(t,cmap=parula_map)\n",
    "    ax[p].spines[\"top\"].set_visible(False)\n",
    "    ax[p].spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax[p].grid(False)\n",
    "\n",
    "    t1,t2 = t[:,:30],np.flip(t[:,31:], axis=1)\n",
    "    #corrp[p] = np.corrcoef(t1.flatten(), t2.flatten())[0,1]\n",
    "    #ax[p].set_title(\"Cluster \"+str(p+1)+\"-\"+str(np.round(corrp[p],2)))\n",
    "plt.tight_layout()\n",
    "#plt.savefig(sp+\"firing_base_pred\"+str(mi)+\".pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr2 = np.corrcoef(filtered_textures.reshape(filtered_textures.shape[0]*filtered_textures.shape[1], filtered_textures.shape[2]).T)\n",
    "\n",
    "# Define a correlation threshold above which images will be considered as 'correlating a lot'\n",
    "corr_threshold = 0.85  # You can adjust this threshold as needed\n",
    "for i in range(10):\n",
    "    # Initialize a list to keep track of indices of images to retain\n",
    "    keep_indices = []\n",
    "    # Initialize a boolean array to mark which images have already been checked\n",
    "    checked = np.zeros(corr2.shape[0], dtype=bool)\n",
    "    # Iterate over the correlation matrix to filter out highly correlated images\n",
    "    for i in range(corr2.shape[0]):\n",
    "        if not checked[i]:  # Only proceed if the image hasn't already been checked\n",
    "            # Mark the current image as checked\n",
    "            checked[i] = True\n",
    "            # Find indices of images that are highly correlated with the current image\n",
    "            high_corr_indices = np.where(corr2[i] > corr_threshold)[0]\n",
    "            # Mark all these correlated images as checked\n",
    "            checked[high_corr_indices] = True\n",
    "            # Mark all highly correlated images as False (discard them)\n",
    "            keep_indices.append(random.sample(list(high_corr_indices),1)[0])\n",
    "\n",
    "    # Use the mask to filter the images\n",
    "    filtered2_textures = filtered_textures[:, :, keep_indices]\n",
    "\n",
    "    print(f\"Original number of images: {filtered_textures.shape[2]}\")\n",
    "    print(f\"Number of images after filtering: {filtered2_textures.shape[2]}\")\n",
    "    print(filtered2_textures.shape)\n",
    "    _, ax = plt.subplots(3, 10, figsize=(figS*3.8, figS))\n",
    "    ax=ax.flatten()\n",
    "    corrp = np.zeros(filtered2_textures.shape[2])\n",
    "    for p in range(filtered2_textures.shape[2]):\n",
    "        #print(p)\n",
    "        t = filtered2_textures[:,:,p]\n",
    "        #print([np.corrcoef(t.flatten(), filtered_textures[:,:,i].flatten())[0,1] for i in range(filtered_textures.shape[2])])\n",
    "        t = np.flip(t.T, axis=0)\n",
    "        ax[p].imshow(t,cmap=parula_map)\n",
    "        ax[p].spines[\"top\"].set_visible(False)\n",
    "        ax[p].spines[\"right\"].set_visible(False)\n",
    "\n",
    "        ax[p].grid(False)\n",
    "\n",
    "        t1,t2 = t[:,:30],np.flip(t[:,31:], axis=1)\n",
    "        corrp[p] = np.corrcoef(t1.flatten(), t2.flatten())[0,1]\n",
    "        ax[p].set_title(\"Cluster \"+str(p+1)+\"-\"+str(np.round(corrp[p],2)))\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(sp+\"firing_base_pred\"+str(mi)+\".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your texture data is loaded and normalized into a variable `textures` (Nx1xHxW tensor)\n",
    "textures = textures[:,:,final_ix]\n",
    "textures = textures.transpose(2, 0, 1).reshape(-1, 1, meanfire1.shape[0], meanfire1.shape[1])\n",
    "\n",
    "print(textures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29bf812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "736df2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_textures = torch.tensor(textures).float()\n",
    "dataset = TensorDataset(tensor_textures)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "k1, k2, k3, out1, out2, out3 = 3, 3, 7, 16, 32, 64\n",
    "def train_model(k1, k2, k3, out1, out2, out3, nE, dataloader, device):\n",
    "    model = ConvAutoencoder(k1, k2, k3, out1, out2, out3).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = nE # Adjust as needed\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in dataloader:\n",
    "            inputs, = data\n",
    "            #print(inputs.shape)\n",
    "            optimizer.zero_grad()\n",
    "            _, decoded = model(inputs)\n",
    "            #print(decoded.shape)\n",
    "            loss = criterion(decoded, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    return model\n",
    "\n",
    "def extract_features_kmean(model, dataloader, nC, rS, textures):\n",
    "    # Use the trained encoder to extract features\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, = data\n",
    "            encoded, _ = model(inputs)\n",
    "            #print(encoded.shape)\n",
    "            features.append(encoded.view(encoded.size(0), -1).cpu().numpy())\n",
    "    sh = encoded.shape\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    #print(features.shape)\n",
    "    # Perform k-means clustering\n",
    "    kmeans = KMeans(n_clusters=nC, random_state=rS).fit(features)\n",
    "    # Calculate the distance of each feature to each cluster center\n",
    "    distances = cdist(features, kmeans.cluster_centers_, 'euclidean')\n",
    "    # Find the index of the closest sample to each cluster center\n",
    "    closest_indices = np.argmin(distances, axis=0)\n",
    "    # Use these indices to select the representative textures from the original dataset\n",
    "    representative_textures = [textures[i] for i in closest_indices]\n",
    "    categories = kmeans.predict(features)\n",
    "    return kmeans, representative_textures, sh, distances, categories\n",
    "\n",
    "def extract_features_hc(model, dataloader, textures, dT):\n",
    "    # Use the trained encoder to extract features\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, = data\n",
    "            encoded, _ = model(inputs)\n",
    "            #print(encoded.shape)\n",
    "            features.append(encoded.view(encoded.size(0), -1).cpu().numpy())\n",
    "    sh = encoded.shape\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    #print(features.shape)\n",
    "    # Perform hierarchical clustering clustering\n",
    "    clustering = AgglomerativeClustering(distance_threshold=dT, n_clusters=None).fit(features)\n",
    "    categories = clustering.labels_\n",
    "    cluster_centers = np.array([features[categories == i].mean(axis=0) for i in np.unique(categories)])\n",
    "    # Calculate the distance of each feature to each cluster center\n",
    "    distances = cdist(features, cluster_centers, 'euclidean')\n",
    "    # Find the index of the closest sample to each cluster center\n",
    "    closest_indices = np.argmin(distances, axis=0)\n",
    "    # Use these indices to select the representative textures from the original dataset\n",
    "    representative_textures = [textures[i] for i in closest_indices]\n",
    "    return clustering, representative_textures, sh, distances, categories, cluster_centers\n",
    "\n",
    "def extract_features_dbsc(model, dataloader, textures, eps, min_samples):\n",
    "    # Use the trained encoder to extract features\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, = data\n",
    "            encoded, _ = model(inputs)\n",
    "            #print(encoded.shape)\n",
    "            features.append(encoded.view(encoded.size(0), -1).cpu().numpy())\n",
    "    sh = encoded.shape\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    #print(features.shape)\n",
    "    # Perform DBSCAN clustering\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan.fit(features)\n",
    "    # DBSCAN labels for the dataset\n",
    "    categories = dbscan.labels_\n",
    "    # Number of clusters\n",
    "    n_clusters_ = len(set(categories)) - (1 if -1 in categories else 0) \n",
    "    cluster_centers = np.array([features[categories == i].mean(axis=0) for i in range(n_clusters_)])\n",
    "    # Calculate the distance of each feature to each cluster center\n",
    "    distances = cdist(features, cluster_centers, 'euclidean')\n",
    "    # Find the index of the closest sample to each cluster center\n",
    "    closest_indices = np.argmin(distances, axis=0)\n",
    "    # Use these indices to select the representative textures from the original dataset\n",
    "    representative_textures = [textures[i] for i in closest_indices]\n",
    "    return dbscan, representative_textures, sh, distances, categories, cluster_centers\n",
    "\n",
    "def generate_textures(kmeans, model, sh, device, cc=None):\n",
    "    if cc is None:\n",
    "        cc = kmeans.cluster_centers_\n",
    "    # Assuming `model` is your trained model and it has a decoder that can take latent space representations\n",
    "    model.eval()\n",
    "    generated_textures = []\n",
    "    with torch.no_grad():\n",
    "        for center in cc:\n",
    "            # Reshape and convert the cluster center to a tensor\n",
    "            #print(center.shape)\n",
    "            center_tensor = torch.tensor(center, dtype=torch.float32).view(sh[1], sh[2], sh[3])  # Adjust shape as necessary\n",
    "            center_tensor = center_tensor.to(device)  # If using GPU\n",
    "            # Decode the center tensor to get the generated texture\n",
    "            generated_texture = model.decoder(center_tensor)\n",
    "            T = np.squeeze(generated_texture.cpu().numpy())\n",
    "            #plt.imshow(T, cmap='gray')\n",
    "            #plt.show()\n",
    "            generated_textures.append(T)\n",
    "    return generated_textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f647f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `features` is the array of encoded features from your dataset\n",
    "# and `kmeans` is your trained KMeans model\n",
    "\n",
    "k1, k2, k3, out1, out2, out3 = 3, 6, 9, 16, 32, 64\n",
    "m = train_model(k1, k2, k3, out1, out2, out3, 50, dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01d0bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save pytorch model\n",
    "torch.save(m, sp+'ClusterModel_sample.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab9b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [60]:\n",
    "\n",
    "    k,texts,sh,dist,c,cc = extract_features_hc(m, dataloader, textures, i)\n",
    "    T = generate_textures(k, m, sh, device,cc)\n",
    "    np.unique(c)\n",
    "    n = np.array([sum(c==j) for j in np.unique(c)])\n",
    "    plt.bar(range(len(np.unique(c))), n, color='blue', alpha=0.7)\n",
    "    plt.show()\n",
    "    print(np.where(n>100))\n",
    "    _, ax = plt.subplots(8, 8, figsize=(figS*8, figS*4))\n",
    "    #ix = [3,5,7,8,9,14,15,17,18,20,22,23,24,26,27,28,30,32,33,34]\n",
    "    corrp = np.zeros(len(T))\n",
    "    ax = ax.ravel()\n",
    "    for p in range(len(T)):\n",
    "        t = T[p]\n",
    "\n",
    "        t = np.flip(t.T, axis=0)\n",
    "        ax[p].imshow(t,cmap=parula_map)\n",
    "        ax[p].spines[\"top\"].set_visible(False)\n",
    "        ax[p].spines[\"right\"].set_visible(False)\n",
    "\n",
    "        ax[p].grid(False)\n",
    "\n",
    "        t1,t2 = t[:,:30],np.flip(t[:,31:], axis=1)\n",
    "        corrp[p] = np.corrcoef(t1.flatten(), t2.flatten())[0,1]\n",
    "        ax[p].set_title(\"Cluster \"+str(p+1)+\"-\"+str(np.round(corrp[p],2)))\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(sp+\"firing_base_pred\"+str(mi)+\".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = np.argsort(corrp)[:15]\n",
    "_, ax = plt.subplots(1, 15, figsize=(figS*15, figS))\n",
    "ax = ax.ravel()\n",
    "for p in range(len(ix)):\n",
    "    t = np.flip(T[ix[p]].T, axis=0)\n",
    "    ax[p].imshow(t,cmap=parula_map)\n",
    "    ax[p].spines[\"top\"].set_visible(False)\n",
    "    ax[p].spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax[p].grid(False)\n",
    "    ax[p].xaxis.set_ticks([])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    #ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    ax[p].yaxis.set_ticks([])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    #ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    #above = 1 if n[p]>100 else 0\n",
    "    ax[p].set_title(\"DS: \"+str(np.round(1-corrp[ix[p]],2)), **f_font1)\n",
    "#plt.tight_layout()\n",
    "plt.savefig(sp+\"clusters_hc.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n1 = len(T)\n",
    "corrp = np.zeros(n1)\n",
    "for p in range(n1):\n",
    "    t = T[p]\n",
    "    t1,t2 = t[:,:45],np.flip(t[:,46:], axis=1)\n",
    "    corrp[p] = np.corrcoef(t1.flatten(), t2.flatten())[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1789630",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1105cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1,n2,n3 = len(T), meanfire.shape[2], meanfire.shape[4]*meanfire.shape[5]*meanfire.shape[6]\n",
    "corr = np.zeros((n1, n2*n2, n3))\n",
    "RMSE = np.zeros((n1, n2*n2, n3))\n",
    "for p in range(n1):\n",
    "    t = T[p]\n",
    "    j=0\n",
    "    for ss in range(5):\n",
    "        for cs in range(5):\n",
    "            fr = meanfire[:,:,ss,cs,:,:,:]\n",
    "            fr = fr.reshape(91,91,-1)\n",
    "            fr = fr.reshape(91*91,-1).T\n",
    "            fr_ = np.array(t.reshape(91*91))\n",
    "            #print(fr.shape, fr_.shape)\n",
    "            corr[p,j,:] = np.corrcoef(fr, fr_)[0,1:]\n",
    "            RMSE[p,j,:] = np.sqrt(np.mean((fr - fr_)**2, axis=1))\n",
    "            j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa52fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ss in range(5):\n",
    "        for cs in range(5):\n",
    "                _, ax = plt.subplots(6, 6, figsize=(figS*4, figS*4))\n",
    "                ax = ax.ravel()\n",
    "                for p in range(n1):\n",
    "                    ax[p].hist(RMSE[p,ss,cs,:], bins=10)\n",
    "                    ax[p].spines[\"top\"].set_visible(False)\n",
    "                    ax[p].spines[\"right\"].set_visible(False)\n",
    "                    ax[p].set_xlim([0, np.max(RMSE)])\n",
    "                    ax[p].grid(False)\n",
    "                    ax[p].set_title(\"Cluster \"+str(p+1))\n",
    "                plt.tight_layout()\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dcd4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = [3,5,7,8,9,14,15,17,18,20,22,23,24,26,27,28,30,32,33,34]\n",
    "_, ax = plt.subplots(4, 5, figsize=(figS*5, figS*4))\n",
    "ax = ax.ravel()\n",
    "for p in range(len(ix)):\n",
    "    t = np.flip(T[ix[p]].T, axis=0)\n",
    "    ax[p].imshow(t,cmap=parula_map)\n",
    "    ax[p].spines[\"top\"].set_visible(False)\n",
    "    ax[p].spines[\"right\"].set_visible(False)\n",
    "\n",
    "    ax[p].grid(False)\n",
    "    ax[p].xaxis.set_ticks([])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    #ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    ax[p].yaxis.set_ticks([])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    #ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    #above = 1 if n[p]>100 else 0\n",
    "    #ax[p].set_title(\"Cluster \"+str(p+1)+\"-\"+str(above))\n",
    "#plt.tight_layout()\n",
    "plt.savefig(sp+\"clusters.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131147ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "k,texts,sh,dist,c,cc = extract_features_dbsc(m, dataloader, textures, 1, 50)\n",
    "T = generate_textures(k, m, sh, device,cc)\n",
    "np.unique(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.array([sum(c==j) for j in np.unique(c)])\n",
    "plt.bar(range(len(np.unique(c))), n, color='blue', alpha=0.7)\n",
    "plt.show()\n",
    "print(np.where(n>100))\n",
    "_, ax = plt.subplots(6, 5, figsize=(figS*1.7, figS*1.5))\n",
    "ax = ax.ravel()\n",
    "for p in range(len(T)):\n",
    "    t = T[p]\n",
    "    ax[p].imshow(t)\n",
    "    ax[p].spines[\"top\"].set_visible(False)\n",
    "    ax[p].spines[\"right\"].set_visible(False)\n",
    "    #ax.spines[\"left\"].set_visible(False)\n",
    "    #ax.spines[\"bottom\"].set_visible(False)\n",
    "    #ax.spines[\"left\"].set_visible(False)\n",
    "    ax[p].grid(False)\n",
    "    # for label in (ax.get_xticklabels()+ax.get_yticklabels()):\n",
    "    #     label.set_fontname(f_font1[\"fontname\"])\n",
    "    #     label.set_fontsize(35)\n",
    "    #ax.xaxis.set_ticks([-4,0,4])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    #ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    #ax.yaxis.set_ticks([-4,0,4])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    #ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    #ax.set_xlim(-4.1, 4.1)\n",
    "    #ax.set_ylim(-4.1, 4.1)\n",
    "    #plt.tick_params(top=False, bottom=False, left=False, right=False,\n",
    "    #                labelleft=False, labelbottom=False)\n",
    "    above = 1 if n[p]>100 else 0\n",
    "    ax[p].set_title(\"Cluster \"+str(p+1)+\"-\"+str(above))\n",
    "plt.tight_layout()\n",
    "#plt.savefig(sp+\"firing_base_pred\"+str(mi)+\".pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f6744",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tk=[]\n",
    "D = []\n",
    "C = []\n",
    "for r in range(5):\n",
    "    print(r)\n",
    "    k,texts,sh,dist,c = extract_features_kmean(m, dataloader, 36, r, textures)\n",
    "    Tk.append(generate_textures(k, m, sh, device))\n",
    "    D.append(dist)\n",
    "    C.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb52880",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec24d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    n = np.array([sum(C[i]==j) for j in np.unique(C[i])])\n",
    "    plt.bar(range(len(np.unique(C[i]))), n, color='blue', alpha=0.7)\n",
    "    plt.show()\n",
    "    print(np.where(n>100))\n",
    "    _, ax = plt.subplots(6, 6, figsize=(figS*3, figS*3))\n",
    "    ax = ax.ravel()\n",
    "    corrp = np.zeros(len(Tk[i]))\n",
    "    for p in range(len(Tk[i])):\n",
    "        t = np.flip(Tk[i][p].T, axis=0)\n",
    "        ax[p].imshow(t,cmap=parula_map)\n",
    "        ax[p].spines[\"top\"].set_visible(False)\n",
    "        ax[p].spines[\"right\"].set_visible(False)\n",
    "        #ax.spines[\"left\"].set_visible(False)\n",
    "        #ax.spines[\"bottom\"].set_visible(False)\n",
    "        #ax.spines[\"left\"].set_visible(False)\n",
    "        ax[p].grid(False)\n",
    "        # for label in (ax.get_xticklabels()+ax.get_yticklabels()):\n",
    "        #     label.set_fontname(f_font1[\"fontname\"])\n",
    "        #     label.set_fontsize(35)\n",
    "        #ax.xaxis.set_ticks([-4,0,4])\n",
    "        #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "        #ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "        #ax.yaxis.set_ticks([-4,0,4])\n",
    "        #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "        #ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "        #ax.set_xlim(-4.1, 4.1)\n",
    "        #ax.set_ylim(-4.1, 4.1)\n",
    "        #plt.tick_params(top=False, bottom=False, left=False, right=False,\n",
    "        #                labelleft=False, labelbottom=False)\n",
    "        t1,t2 = t[:,:30],np.flip(t[:,31:], axis=1)\n",
    "        corrp[p] = np.corrcoef(t1.flatten(), t2.flatten())[0,1]\n",
    "        ax[p].set_title(\"Cluster \"+str(p+1)+\"-\"+str(np.round(corrp[p],2)))\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(sp+\"firing_base_pred\"+str(mi)+\".pdf\")\n",
    "    plt.show()\n",
    "\n",
    "    ix = np.argsort(corrp)[:15]\n",
    "    _, ax = plt.subplots(1, 15, figsize=(figS*15, figS))\n",
    "    ax = ax.ravel()\n",
    "    for p in range(len(ix)):\n",
    "        t = np.flip(Tk[i][ix[p]].T, axis=0)\n",
    "        ax[p].imshow(t,cmap=parula_map)\n",
    "        ax[p].spines[\"top\"].set_visible(False)\n",
    "        ax[p].spines[\"right\"].set_visible(False)\n",
    "\n",
    "        ax[p].grid(False)\n",
    "        ax[p].xaxis.set_ticks([])\n",
    "        #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "        #ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "        ax[p].yaxis.set_ticks([])\n",
    "        #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "        #ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "        #above = 1 if n[p]>100 else 0\n",
    "        ax[p].set_title(\"DS: \"+str(np.round(1-corrp[ix[p]],2)), **f_font1)\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig(sp+\"clusters_km\"+str(i)+\".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e2762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(sp+\"kmean_36cluster_data.npz\", Tk=Tk, D=D, C=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n1,n2,n3 = len(Tk[3]), meanfire.shape[2], meanfire.shape[4]*meanfire.shape[5]*meanfire.shape[6]\n",
    "corr = np.zeros((n1, n2*n2, n3))\n",
    "RMSE = np.zeros((n1, n2*n2, n3))\n",
    "for p in range(n1):\n",
    "    t = Tk[3][p]\n",
    "    j=0\n",
    "    for ss in range(4,-1,-1):\n",
    "        print(ss)\n",
    "        for cs in range(5):\n",
    "            fr = meanfire[:,:,ss,cs,:,:,:]\n",
    "            fr = fr.reshape(91,91,-1)\n",
    "            fr = fr.reshape(91*91,-1).T\n",
    "            fr_ = np.array(t.reshape(91*91))\n",
    "            #print(fr.shape, fr_.shape)\n",
    "            corr[p,j,:] = np.corrcoef(fr, fr_)[-1,1:]\n",
    "            #print(np.corrcoef(fr, fr_)[-1,1:])\n",
    "            RMSE[p,j,:] = np.sqrt(np.mean((fr - fr_)**2, axis=1))\n",
    "            j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6373d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrp = np.zeros(len(Tk[3]))\n",
    "for p in range(len(Tk[3])):\n",
    "    t = np.flip(Tk[3][p].T, axis=0)\n",
    "    t1,t2 = t[:,:45],np.flip(t[:,46:], axis=1)\n",
    "    corrp[p] = np.corrcoef(t1.flatten(), t2.flatten())[0,1]\n",
    "\n",
    "ix = np.argsort(corrp)[:15]\n",
    "\n",
    "_, ax = plt.subplots(1, len(ix), figsize=(figS*15, figS*1))\n",
    "ax = ax.ravel()\n",
    "for p in range(len(ix)):\n",
    "    ax[p].errorbar(range(1,n2*n2+1),np.mean(RMSE[ix[p],:,:],axis=1), yerr=np.std(RMSE[ix[p],:,:],axis=1))\n",
    "    ax[p].spines[\"top\"].set_visible(False)\n",
    "    ax[p].spines[\"right\"].set_visible(False)\n",
    "    ax[p].set_ylim([0, np.max(RMSE)])\n",
    "    ax[p].grid(False)\n",
    "    #ax[p].set_title(\"Cluster \"+str(p+1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(sp+\"clusters_km3_diag_rmse.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrp = np.zeros(len(Tk[3]))\n",
    "for p in range(len(Tk[3])):\n",
    "    t = np.flip(Tk[3][p].T, axis=0)\n",
    "    t1,t2 = t[:,:45],np.flip(t[:,46:], axis=1)\n",
    "    corrp[p] = np.corrcoef(t1.flatten(), t2.flatten())[0,1]\n",
    "\n",
    "ix = np.argsort(corrp)[:15]\n",
    "\n",
    "_, ax = plt.subplots(1, len(ix), figsize=(figS*15, figS*1))\n",
    "ax = ax.ravel()\n",
    "for p in range(len(ix)):\n",
    "    ax[p].errorbar(range(1,n2*n2+1),np.mean(corr[ix[p],:,:],axis=1), yerr=np.std(corr[ix[p],:,:],axis=1),linewidth=5)\n",
    "    ax[p].spines[\"top\"].set_visible(False)\n",
    "    ax[p].spines[\"right\"].set_visible(False)\n",
    "    ax[p].set_ylim([0, np.max(corr)])\n",
    "    ax[p].grid(False)\n",
    "    ax[p].set_ylim([0,1])\n",
    "    ax[p].set_xlim([0,n2*n2+1])\n",
    "    ax[p].yaxis.set_ticks([0,0.5,1])\n",
    "    ax[p].yaxis.set_ticklabels([0,0.5,1], **f_font1)\n",
    "    ax[p].xaxis.set_ticks([0,5,10,15,20,25])\n",
    "    ax[p].xaxis.set_ticklabels([1,5,10,15,20,25], **f_font1)\n",
    "    #ax[p].set_title(\"Cluster \"+str(p+1))\n",
    "plt.tight_layout()\n",
    "plt.savefig(sp+\"clusters_km3_diag_corr.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ba5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5,30,5):\n",
    "    plt.hist(C[i], bins=20)\n",
    "    plt.show()\n",
    "    # for j in range(len(T[i])):\n",
    "    #     print(i,j,np.mean(D[i][:,j]))\n",
    "    #     plt.hist(D[i][:,j], bins=20)\n",
    "    #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd522398",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    _, ax = plt.subplots(5, 4, figsize=(figS*4, figS*5))\n",
    "    ax = ax.ravel()\n",
    "    for p in range(len(T[i])):\n",
    "        t = T[i][p]\n",
    "        ax[p].imshow(t)\n",
    "        ax[p].spines[\"top\"].set_visible(False)\n",
    "        ax[p].spines[\"right\"].set_visible(False)\n",
    "        #ax.spines[\"left\"].set_visible(False)\n",
    "        #ax.spines[\"bottom\"].set_visible(False)\n",
    "        #ax.spines[\"left\"].set_visible(False)\n",
    "        ax[p].grid(False)\n",
    "        # for label in (ax.get_xticklabels()+ax.get_yticklabels()):\n",
    "        #     label.set_fontname(f_font1[\"fontname\"])\n",
    "        #     label.set_fontsize(35)\n",
    "        #ax.xaxis.set_ticks([-4,0,4])\n",
    "        #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "        #ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "        #ax.yaxis.set_ticks([-4,0,4])\n",
    "        #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "        #ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "        #ax.set_xlim(-4.1, 4.1)\n",
    "        #ax.set_ylim(-4.1, 4.1)\n",
    "        #plt.tick_params(top=False, bottom=False, left=False, right=False,\n",
    "        #                labelleft=False, labelbottom=False)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(sp+\"firing_base_pred\"+str(mi)+\".pdf\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf3600",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.zeros((textures.shape[0], len(T), len(T[-1])))\n",
    "for k in range(textures.shape[0]):\n",
    "    for i in range(len(T)):\n",
    "        for j in range(len(T[i])):\n",
    "            d = np.linalg.norm(textures[k]-T[i][j])\n",
    "            N[k, i, j] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f004a7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(T)):\n",
    "    for j in range(len(T[i])):\n",
    "        print(i,j,np.mean(N[:,i,j]))\n",
    "        plt.hist(N[:,i,j], bins=20)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74337f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = dir_post.shape[1]\n",
    "xtck = [0,int(np.round(l/4)),int(np.round(l/2)),int(np.round(l*3/4)),int(np.round(l))]\n",
    "xtckL = [\"-90\",\"-45\",\"0\",\"45\",\"90\"]\n",
    "ytck = [-20,0,20,40]\n",
    "ytckL = [\"-90\",\"-45\",\"0\",\"45\",\"90\"]\n",
    "x_eval = np.linspace(-180, 180, num=1000)\n",
    "deg45 = int(np.round(l/2))\n",
    "x = np.arange(2,l-2,4)\n",
    "xL = [str(round(i)) for i in x*360/l-180]\n",
    "_, ax = plt.subplots(1, 1, figsize=(figS*1.2, figS*.66))\n",
    "kde = gaussian_kde(dir_post[:,deg45,x[3],2,2,1])\n",
    "print(\"Speed: 7cps\")\n",
    "print(\"direction: center=0, surround=\", xL[3])\n",
    "Y = kde(x_eval)\n",
    "X = x_eval.copy()\n",
    "ax.plot(X,Y,  color=\"royalblue\", linewidth=line_w+10)\n",
    "yy = norm.pdf(x_eval, loc=X[np.argmax(Y)], scale=2.9)\n",
    "ax.plot(X,yy*0.55, \"--\", color=\"darkorange\", linewidth=line_w+10)\n",
    "yy = norm.pdf(x_eval, loc=X[np.argmax(Y[:550])], scale=3.)\n",
    "ax.plot(X,yy*0.45, \"--\", color=\"darkkhaki\", linewidth=line_w+10)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_linewidth(line_w)\n",
    "ax.spines[\"bottom\"].set_linewidth(line_w)\n",
    "ax.spines[\"left\"].set_linewidth(line_w)\n",
    "ax.grid(False)\n",
    "for label in (ax.get_xticklabels()+ax.get_yticklabels()):\n",
    "    label.set_fontname(f_font1[\"fontname\"])\n",
    "    label.set_fontsize(35)\n",
    "ax.xaxis.set_ticks(ytck)\n",
    "ax.yaxis.set_ticks([0,0.05, 0.1])\n",
    "ax.xaxis.set_ticklabels(ytck)\n",
    "ax.set_xlim(-20, 40)\n",
    "ax.set_ylim(-0.001, 0.1)\n",
    "ax.yaxis.set_ticklabels([0,0.05, 0.1])\n",
    "#ax.set_xlabel(\" \")\n",
    "#ax.set_ylabel(L[i], **f_font2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(sp+\"1PostDir_mix_subj2_rel.pdf\")\n",
    "plt.show()\n",
    "print(X[np.argmax(Y)],X[np.argmax(Y[:550])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ee40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(figS*0.5, figS*0.5))\n",
    "axins = ax\n",
    "axins.set_xlabel(\" \", **f_font1)\n",
    "axins.xaxis.set_ticks([ ])\n",
    "axins.set_ylim([-0.1,1.2])\n",
    "axins.yaxis.set_ticks([0,0.5,1])\n",
    "y = 0.5*(0.9999997527657009*0.7992650300262982)+0.5*(0.014605305925411264*0.6659689207434096)\n",
    "ye = (0.5*(0.15219079588881168+0.24989533811151143)+0.5*(0.2999999629148551+0.2698897545039447))/2\n",
    "axins.errorbar(1,y, yerr=np.array([[ye],[ye]]), fmt='-o', color=c[1], capsize=15, capthick=5, markersize=20, linewidth=line_w+8)\n",
    "axins.spines[\"right\"].set_visible(False)\n",
    "axins.spines[\"top\"].set_visible(False)\n",
    "#axins.spines[\"bottom\"].set_visible(False)\n",
    "axins.grid(False)\n",
    "axins.set_ylabel(\" \", **f_font1)\n",
    "#axins.yaxis.set_ticklabels([])\n",
    "for labl in (axins.get_xticklabels()+axins.get_yticklabels()):\n",
    "        labl.set_fontname(f_font1[\"fontname\"])\n",
    "        labl.set_fontsize(f_font1[\"size\"])\n",
    "plt.tight_layout()\n",
    "plt.savefig(sp+\"fire_example34_\"+str(i)+\".pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540149b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(figS*1.2, figS))\n",
    "M = [[-0.5,-0.5],\n",
    "    [0,-0.5],\n",
    "    [0.5,-0.5],\n",
    "    [-0.5,0],\n",
    "    [0,0],\n",
    "    [0.5,0],\n",
    "    [-0.5,0.5],\n",
    "    [0,0.5],\n",
    "    [0.5,0.5]\n",
    "]\n",
    "\n",
    "R = [0.07, 0.15, 0.3]\n",
    "ticks =[-1, -0.5, 0, 0.5, 1]\n",
    "x = np.linspace(-4,4,10000)\n",
    "col = [\"saddlebrown\", \"firebrick\",  \"tomato\"]\n",
    "for m in M:\n",
    "    ax.plot(m[0], m[1], \"ko\", markersize=7)\n",
    "    for i,r in enumerate(R):\n",
    "        ax.plot(m[0]+r*np.cos(x) , m[1]+r*np.sin(x), \"-\", linewidth=line_w+2, color=col[i])\n",
    "#ax.contour(x, y, Y, levels=3, colors=[\"saddlebrown\", \"tomato\",\"firebrick\",  ], linewidths=line_w+2,  alpha=0.7)\n",
    "c, c_ = np.array([[0.05, 0], [0,0.05]]), np.array([[0.06, 0.1], [0.04,0.1]])\n",
    "W = [0, 0.9, 0.1, 0.9, 0.1, 0, 0, 0.9, 0]\n",
    "Y, Y_ = 0, 0\n",
    "x, y = np.mgrid[-1:1:.001, -1:1:.001]\n",
    "pos = np.empty(x.shape + (2,))\n",
    "pos[:, :, 0] = x\n",
    "pos[:, :, 1] = y\n",
    "for w,m in zip(W,M):\n",
    "    Y = Y + w * multivariate_normal.pdf(pos, mean=m, cov=c)\n",
    "for w,m in zip(W,M):\n",
    "    Y_ = Y_ + w * multivariate_normal.pdf(pos, mean=m, cov=c_)\n",
    "ax.contour(x, y, Y_, levels=3, linewidths=line_w+4, )\n",
    "# for m in [[0,-0.5],[-0.5,0],[0,0.5],]:\n",
    "#     ax.plot(m[0], m[1], \"ko\", markersize=7)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_linewidth(line_w)\n",
    "ax.spines[\"bottom\"].set_linewidth(line_w)\n",
    "ax.spines[\"left\"].set_linewidth(line_w)\n",
    "ax.grid(False)\n",
    "for label in (ax.get_xticklabels()+ax.get_yticklabels()):\n",
    "    label.set_fontname(f_font1[\"fontname\"])\n",
    "    label.set_fontsize(35)\n",
    "ax.xaxis.set_ticks(ticks)\n",
    "ax.yaxis.set_ticks(ticks)\n",
    "#ax.xaxis.set_ticklabels([\" \", \" \", \" \"], rotation = -60)\n",
    "#ax.set_xlim(-0.5, 2.5)\n",
    "#ax.yaxis.set_ticks(yax)\n",
    "#ax.set_xlabel(\" \")\n",
    "#ax.set_ylabel(L[i], **f_font2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(sp+\"concept_testPostVv2.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(figS*2, figS))\n",
    "c = [\"royalblue\", \"olivedrab\", \"darkorange\", \"firebrick\"]\n",
    "x = np.linspace(-4.8,4,1000)\n",
    "lw = 10\n",
    "y = norm.pdf(x, loc=-2.7, scale=0.7)\n",
    "ax.plot(x, y,  '-', linewidth=line_w+lw, color = c[1])\n",
    "y = norm.pdf(x, loc=0.3, scale=0.7)\n",
    "ax.plot(x, y,  '-', linewidth=line_w+lw, color = c[2])\n",
    "y = norm.pdf(x, loc=2.7, scale=0.7)\n",
    "ax.plot(x, y,  '-', linewidth=line_w+lw, color = c[3])\n",
    "y = np.sum([0.6*norm.pdf(x, loc=-2.7, scale=0.7), 0.4*norm.pdf(x, loc=0.3, scale=1)], axis=0)\n",
    "ax.plot(x, y,  '-', linewidth=line_w+lw, color = c[0])\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.grid(False)\n",
    "# for label in (ax.get_xticklabels()+ax.get_yticklabels()):\n",
    "#     label.set_fontname(f_font1[\"fontname\"])\n",
    "#     label.set_fontsize(35)\n",
    "#ax.xaxis.set_ticks(range(1,len(M)+1))\n",
    "#ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "#ax.xaxis.set_ticklabels([\"#\"+str(i+1) for i in range(len(M))])\n",
    "#ax.set_xlim(0, 10)\n",
    "#ax.set_ylim(-0.03, 0.4)\n",
    "#ax.yaxis.set_ticks(yax)\n",
    "#ax.set_xlabel(\" \")\n",
    "#ax.set_ylabel(L[i], **f_font2)\n",
    "plt.tick_params(top=False, bottom=False, left=False, right=False,\n",
    "                labelleft=False, labelbottom=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(sp+\"concept_mixture_1d.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3facfb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [\"royalblue\", \"olivedrab\", \"darkorange\", \"firebrick\"]\n",
    "x, y = np.mgrid[-4:4:.01, -4:4:.01]\n",
    "pos = np.empty(x.shape + (2,))\n",
    "pos[:, :, 0] = x\n",
    "pos[:, :, 1] = y\n",
    "lw = 10\n",
    "CM = [\"Purples\", \"Blues\", \"Greens\", \"Oranges\", \"Reds\"]\n",
    "for mi,m,c in zip(range(3),[[-1,1],[0,0],[1,-1]], [np.array([[2, 1.5], [1.5, 2]]), np.array([[1.9, 1.2], [1.2,1.9]]), np.array([[1.9, 1.5], [1.2,1.9]])]):\n",
    "    _, ax = plt.subplots(1, 1, figsize=(figS*0.5, figS*0.5))\n",
    "    Y_ = multivariate_normal.pdf(pos, mean=m, cov=c)\n",
    "    ax.contour(x, y, Y_, levels=4, linewidths=line_w+4, cmap=CM[mi])\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    #ax.spines[\"left\"].set_visible(False)\n",
    "    #ax.spines[\"bottom\"].set_visible(False)\n",
    "    #ax.spines[\"left\"].set_visible(False)\n",
    "    ax.grid(False)\n",
    "    # for label in (ax.get_xticklabels()+ax.get_yticklabels()):\n",
    "    #     label.set_fontname(f_font1[\"fontname\"])\n",
    "    #     label.set_fontsize(35)\n",
    "    ax.xaxis.set_ticks([-4,0,4])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    ax.yaxis.set_ticks([-4,0,4])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    ax.set_xlim(-4.1, 4.1)\n",
    "    ax.set_ylim(-4.1, 4.1)\n",
    "    #plt.tick_params(top=False, bottom=False, left=False, right=False,\n",
    "    #                labelleft=False, labelbottom=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(sp+\"firing_base_pred\"+str(mi)+\".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [[0.7, 0.2, 0.05],\n",
    "     [0.12, 0.081, 0.8]]\n",
    "mij,mj,cj = 0, [-1.5,1.5], np.array([[1, 0.3], [0.3,1]])\n",
    "_, ax = plt.subplots(1, 1, figsize=(figS*0.5, figS*0.5))\n",
    "for mi,m,c in zip(range(3),[[1,-1],[0,0],[-1,1]], [np.array([[1.9, 1.5], [1.2,1.9]]), np.array([[1.9, 1.2], [1.2,1.9]]),np.array([[2, 1.5], [1.5, 2]]) ]):\n",
    "    Y_ = multivariate_normal.pdf(pos, mean=m, cov=c)\n",
    "    ax.contour(x, y, Y_*w[mij][mi], levels=3, linewidths=line_w+4, cmap=CM[mi],vmin=0, vmax=0.05)\n",
    "#Y_ = multivariate_normal.pdf(pos, mean=mj, cov=cj)\n",
    "#ax.contour(x, y, Y_, levels=3, linewidths=line_w+4, cmap=\"Blues\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "#ax.spines[\"left\"].set_visible(False)\n",
    "#ax.spines[\"bottom\"].set_visible(False)\n",
    "#ax.spines[\"left\"].set_visible(False)\n",
    "ax.grid(False)\n",
    "# for label in (ax.get_xticklabels()+ax.get_yticklabels()):\n",
    "#     label.set_fontname(f_font1[\"fontname\"])\n",
    "#     label.set_fontsize(35)\n",
    "ax.xaxis.set_ticks([-4,0,4])\n",
    "#ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "ax.yaxis.set_ticks([-4,0,4])\n",
    "#ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "ax.set_xlim(-4.1, 4.1)\n",
    "ax.set_ylim(-4.1, 4.1)\n",
    "#plt.tick_params(top=False, bottom=False, left=False, right=False,\n",
    "#                labelleft=False, labelbottom=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(sp+\"firing_test_mix_pred1\"+str(mij)+\".pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [[0.7, 0.2, 0.05],\n",
    "     [0.25, 0.1, 0.8]]\n",
    "for mij,mj,cj in zip(range(2),[[-1.5,1.5],[1.5,-1.5]], [np.array([[1, 0.3], [0.3,1]]), np.array([[1, 0.3], [0.3,1]])]):\n",
    "    _, ax = plt.subplots(1, 1, figsize=(figS*0.5, figS*0.5))\n",
    "    for mi,m,c in zip(range(3),[[-1,1],[0,0],[1,-1]], [np.array([[2, 1.5], [1.5, 2]]), np.array([[1.9, 1.2], [1.2,1.9]]), np.array([[1.9, 1.5], [1.2,1.9]])]):\n",
    "        Y_ = multivariate_normal.pdf(pos, mean=m, cov=c)\n",
    "        ax.contour(x, y, Y_*w[mij][mi], levels=3, linewidths=line_w+4, cmap=CM[mi],vmin=0, vmax=0.05)\n",
    "    #Y_ = multivariate_normal.pdf(pos, mean=mj, cov=cj)\n",
    "    #ax.contour(x, y, Y_, levels=3, linewidths=line_w+4, cmap=\"Blues\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    #ax.spines[\"left\"].set_visible(False)\n",
    "    #ax.spines[\"bottom\"].set_visible(False)\n",
    "    #ax.spines[\"left\"].set_visible(False)\n",
    "    ax.grid(False)\n",
    "    # for label in (ax.get_xticklabels()+ax.get_yticklabels()):\n",
    "    #     label.set_fontname(f_font1[\"fontname\"])\n",
    "    #     label.set_fontsize(35)\n",
    "    ax.xaxis.set_ticks([-4,0,4])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    ax.yaxis.set_ticks([-4,0,4])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    ax.set_xlim(-4.1, 4.1)\n",
    "    ax.set_ylim(-4.1, 4.1)\n",
    "    #plt.tick_params(top=False, bottom=False, left=False, right=False,\n",
    "    #                labelleft=False, labelbottom=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(sp+\"firing_test_mix_pred1\"+str(mij)+\".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mi,m,c in zip(range(2),[[-1.5,1.5],[1.5,-1.5]], [np.array([[1, 0.3], [0.3,1]]), np.array([[1, 0.3], [0.3,1]])]):\n",
    "    _, ax = plt.subplots(1, 1, figsize=(figS*0.5, figS*0.5))\n",
    "    Y_ = multivariate_normal.pdf(pos, mean=m, cov=c)\n",
    "    ax.contour(x, y, Y_, levels=3, linewidths=line_w+4, cmap=\"Blues\",vmin=0, vmax=0.15)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    #ax.spines[\"left\"].set_visible(False)\n",
    "    #ax.spines[\"bottom\"].set_visible(False)\n",
    "    #ax.spines[\"left\"].set_visible(False)\n",
    "    ax.grid(False)\n",
    "    # for label in (ax.get_xticklabels()+ax.get_yticklabels()):\n",
    "    #     label.set_fontname(f_font1[\"fontname\"])\n",
    "    #     label.set_fontsize(35)\n",
    "    ax.xaxis.set_ticks([-4,0,4])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    ax.yaxis.set_ticks([-4,0,4])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    ax.set_xlim(-4.1, 4.1)\n",
    "    ax.set_ylim(-4.1, 4.1)\n",
    "    #plt.tick_params(top=False, bottom=False, left=False, right=False,\n",
    "    #                labelleft=False, labelbottom=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(sp+\"firing_test_pred1\"+str(mi)+\".pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e864f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.mean(Y_*w[mij][mi], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e10998",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [[0.7, 0.2, 0.05],\n",
    "     [0.12, 0.081, 0.8]]\n",
    "for mij,mj,cj in zip(range(2),[[-1.5,1.5],[1.5,-1.5]], [np.array([[1, 0.3], [0.3,1]]), np.array([[1, 0.3], [0.3,1]])]):\n",
    "    _, ax = plt.subplots(1, 1, figsize=(figS*0.5, figS*0.5))\n",
    "    for mi,m,c in zip(range(3),[[-2,2],[0,0],[2,-2]], [np.array([[1, 1.4], [1, 1.4]]), np.array([[0.9, 0.2], [0.2,0.9]]), np.array([[1.2, 1.5], [1.2,1.5]])]):\n",
    "        Y_ = multivariate_normal.pdf(pos, mean=m, cov=c)\n",
    "        ax.contour(x, y, Y_*w[mij][mi], levels=3, linewidths=line_w+3, cmap=\"Greys\",vmin=0, vmax=0.05)\n",
    "    #Y_ = multivariate_normal.pdf(pos, mean=mj, cov=cj)\n",
    "    #ax.contour(x, y, Y_, levels=3, linewidths=line_w+4, cmap=\"Blues\")\n",
    "    de = 1\n",
    "    if mij==0:\n",
    "        x0 = np.mean(np.mean(Y_*w[mij][mi], axis=0))-de\n",
    "        y0=np.mean(np.mean(Y_*w[mij][mi], axis=1))+de\n",
    "        ax.plot(x0, y0, \"ro\", markersize=12)\n",
    "        # Define the semi-major and semi-minor axes\n",
    "        a = 2.4  # length of semi-major axis\n",
    "        b = 2  # length of semi-minor axis\n",
    "    else:\n",
    "        x0=np.mean(np.mean(Y_*w[mij][mi], axis=0))+de-0.3\n",
    "        y0=np.mean(np.mean(Y_*w[mij][mi], axis=1))-de+0.3\n",
    "        ax.plot(x0,y0, \"ro\", markersize=12)\n",
    "        # Define the semi-major and semi-minor axes\n",
    "        a = 3  # length of semi-major axis\n",
    "        b = 2.1  # length of semi-minor axis\n",
    "    \n",
    "\n",
    "    # Generate angle values\n",
    "    theta = np.linspace(0, 2 * np.pi, 100)\n",
    "\n",
    "    # Parametric equation of the ellipse\n",
    "    xe = a * np.cos(theta) + x0\n",
    "    ye = b * np.sin(theta) + y0\n",
    "\n",
    "    phi = np.radians(-45) \n",
    "    # Apply rotation to the ellipse\n",
    "    x_rotated = (xe - x0) * np.cos(phi) - (ye - y0) * np.sin(phi) + x0\n",
    "    y_rotated = (xe - x0) * np.sin(phi) + (ye - y0) * np.cos(phi) + y0\n",
    "\n",
    "    ax.plot(x_rotated, y_rotated, \"r--\", linewidth=3)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    #ax.spines[\"left\"].set_visible(False)\n",
    "    #ax.spines[\"bottom\"].set_visible(False)\n",
    "    #ax.spines[\"left\"].set_visible(False)\n",
    "    ax.grid(False)\n",
    "    # for label in (ax.get_xticklabels()+ax.get_yticklabels()):\n",
    "    #     label.set_fontname(f_font1[\"fontname\"])\n",
    "    #     label.set_fontsize(35)\n",
    "    ax.xaxis.set_ticks([-4,0,4])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    ax.xaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    ax.yaxis.set_ticks([-4,0,4])\n",
    "    #ax.yaxis.set_ticks([0,0.1,0.2,0.3, 0.4])\n",
    "    ax.yaxis.set_ticklabels([\"0\",\"0.5\",\"1\"], **f_font1)\n",
    "    ax.set_xlim(-4.1, 4.1)\n",
    "    ax.set_ylim(-4.1, 4.1)\n",
    "    #plt.tick_params(top=False, bottom=False, left=False, right=False,\n",
    "    #                labelleft=False, labelbottom=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(sp+\"firing_test_mix_pred\"+str(mij)+\".pdf\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
